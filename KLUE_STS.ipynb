{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722e2172",
   "metadata": {},
   "source": [
    "# Sentence Transformers 학습과 활용\n",
    "\n",
    "본 노트북에서는 klue/roberta-base 모델을 KLUE 내 STS 데이터셋을 활용하여 모델을 훈련하는 예제를 다루게 됩니다. [데이터셋](https://github.com/KLUE-benchmark/KLUE \"데이터셋\")<br>\n",
    "\n",
    "학습을 통해 얻어질 sentence-klue-roberta-base 모델은 입력된 문장의 임베딩을 계산해 유사도를 예측하는데 사용할 수 있게 됩니다. <br>\n",
    "\n",
    "학습 과정 이후에는 간단한 예제 코드를 통해 모델이 어떻게 활용되는지도 함께 알아보도록 할 것입니다.\n",
    "\n",
    "모든 소스코드는 [sentence-transformers](\"https://github.com/UKPLab/sentence-transformers\" \"sentence-transformers\") 원 라이브러리를 참고하였습니다. <br>\n",
    "\n",
    "학습 내용의 대부분은 [Klue-transformers-tutorial](https://github.com/Huffon/klue-transformers-tutorial/blob/master/sentence_transformers.ipynb \"Klue-transformers-tutoral\") 을 참고하였으며, tutorial의 내용 중 제가 공부하면서 이해가 필요한 내용을 보충하였습니다.<br>\n",
    "\n",
    "notebook의 환경은 ainize workspace 입니다. [ainize](https://ainize.ai/ \"ainize\") 에서 오른쪽 상단 \"Join with Github\"를 클릭해서 가입한 후 My space에서 workspace를 생성하실 수 있습니다. <br>\n",
    "\n",
    "먼저 노트북을 설정하는데 필요한 라이브러리를 설치합니다. 모델 훈련을 위해서는 sentence-transformers가, 학습 데이터셋 로드를 위해서는 datasets 라이브러리의 설치가 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50704ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 30.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.18.5)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.0.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 35.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.9)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.6.1-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow<4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.7 MB 22.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.9.1+cu111)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.4.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 39.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 43.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.7.1-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 30.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 35.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.1.2)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=321db3e0fb7be522b63ed960a9b16098fdb89c06af1ca82eb68f9cd68379e70e\n",
      "  Stored in directory: /workspace/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tqdm, regex, joblib, click, tokenizers, threadpoolctl, sacremoses, huggingface-hub, dill, xxhash, transformers, sentencepiece, scikit-learn, pyarrow, nltk, multiprocess, fsspec, sentence-transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.60.0\n",
      "    Uninstalling tqdm-4.60.0:\n",
      "      Successfully uninstalled tqdm-4.60.0\n",
      "Successfully installed click-8.0.1 datasets-1.8.0 dill-0.3.4 fsspec-2021.6.1 huggingface-hub-0.0.12 joblib-1.0.1 multiprocess-0.70.12.2 nltk-3.6.2 pyarrow-3.0.0 regex-2021.7.1 sacremoses-0.0.45 scikit-learn-0.24.2 sentence-transformers-2.0.0 sentencepiece-0.1.96 threadpoolctl-2.1.0 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.8.2 xxhash-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76471a81",
   "metadata": {},
   "source": [
    "학습 경과를 지켜보는데 사용될 logger를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee2c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ded90a",
   "metadata": {},
   "source": [
    "학습에 필요한 정보를 변수로 기록합니다.<br>\n",
    "\n",
    "본 노트북에서는 Klue-roberta-base 모델을 활용하지만, https://huggingface.co/klue 페이지에서 더 다양한 사전학습 언어모델을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778d4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a02219",
   "metadata": {},
   "source": [
    "모델 정보 외에도 학습에 필요한 하이퍼 파라미터를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd6cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32 #전체 데이터를 나누는 batch_size\n",
    "num_epochs = 4 # 전체 데이터 학습 횟수\n",
    "model_save_path = \"output/training_klue_sts_\" + model_name.replace(\"/\",\"-\")+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b21243",
   "metadata": {},
   "source": [
    "앞서 정의한 사전학습 언어 모델을 로드합니다.<br>\n",
    "\n",
    "sentence-transformers는 HuggingFace의 transformers와 호환이 잘 이루어지고 있기 때문에,<br>\n",
    "\n",
    "[모델 허브](https://huggingface.co/models \"모델 허브\")에 올라와있는 대부분의 언어 모델을 임베딩을 추출할 Embedder 로 사용할 수 있습니다. \n",
    "<br>\n",
    "\n",
    "모델을 불러오면 경고가 발생하는데, RobertaModel을 fine-tuning 하기 전에는 모델읠 성능이 좋지 않을 것이라고 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79461b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056baae68b6e45f1915af2c1b1d87d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=546.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5647a43ae5043bfb9c9ef211ca99718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442653775.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922d14f207724916ab4f06c9d8f15c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=337.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e1af27fa0e4cc69960cd37b519b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=248477.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d511011e5404607be27e030f8aceee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=173.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models.Transformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b51219",
   "metadata": {},
   "source": [
    "Embedder 에서 추출된 토큰 단위 임베딩들을 가지고 문장 임베딩을 어떻게 계산할 것인지를 결정하는 Pooler를 정의합니다.\n",
    "Word Embedding에 대한 정보는 [여기](https://www.youtube.com/watch?v=MiKh_vEZcTY \"여기\") <br>\n",
    "\n",
    "여러 Pooling 기법이 있겠지만, 예제 노트북에서는 Mean Pooling을 사용하기로 합니다.\n",
    "Mean Pooling이란 모델이 반환한 모든 토큰 임베딩을 더해준 후, 더해진 토큰 개수만큼 나누어 문장을 대표하는 임베딩으로 사용하는 기법을 의미합니다.\n",
    "[Pooling](https://hobinjeong.medium.com/cnn%EC%97%90%EC%84%9C-pooling%EC%9D%B4%EB%9E%80-c4e01aa83c83 \"Pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c11ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False, #mean Pooling을 이용하기 때문에 cls_token 과 max_token은 False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79a44c",
   "metadata": {},
   "source": [
    "Embedder와 Pooler 를 정의했으므로, 이 두 모듈로 구성된 하나의 모델을 정의합니다.<br>\n",
    "\n",
    "modules에 입력으로 들어가는 모듈이 순차적으로 임베딩 과정에 사용이 된다고 생각하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ac2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(modules=[embedding_model,pooler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5c828",
   "metadata": {},
   "source": [
    "이제 학습에 사용될 KLUE STS 데이터셋을 다운로드 및 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d84a141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeaba391fdf4b59b8693fffc1df9ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5191.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8154cb4ef44189802b9d4c6e551264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2932.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset klue/sts (download: 1.29 MiB, generated: 2.82 MiB, post-processed: Unknown size, total: 4.11 MiB) to /workspace/.cache/huggingface/datasets/klue/sts/1.0.0/55ff8f92b7a4b9842be6514ce0b4b5295b46d5e493f8bb5760da4be717018f90...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9796e10520d34193985daf9865f0219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1349875.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset klue downloaded and prepared to /workspace/.cache/huggingface/datasets/klue/sts/1.0.0/55ff8f92b7a4b9842be6514ce0b4b5295b46d5e493f8bb5760da4be717018f90. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"klue\",\"sts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b49ab",
   "metadata": {},
   "source": [
    "다운로드 혹은 로드 후 얻어진 datasets 객체를 살펴보면, 훈련 데이터와 검증 데이터가 포함되어 있는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8640df6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'labels', 'sentence1', 'sentence2', 'source'],\n",
       "        num_rows: 11668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'labels', 'sentence1', 'sentence2', 'source'],\n",
       "        num_rows: 519\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f83b16",
   "metadata": {},
   "source": [
    "각 예시 데이터는 아래와 같이 두개의 문장과 두 문장의 유사도를 라벨로 지니고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df6e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'klue-sts-v1_train_00000',\n",
       " 'labels': {'label': 3.7, 'real-label': 3.714285714285714, 'binary-label': 1},\n",
       " 'sentence1': '숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.',\n",
       " 'sentence2': '숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.',\n",
       " 'source': 'airbnb-rtt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736ab48",
   "metadata": {},
   "source": [
    "이제 테스트에 활용할 데이터를 얻어야 할 차례입니다. <br>\n",
    "\n",
    "위에서 살펴본 바와 같이 KLUE 내 STS 데이터셋은 테스트셋을 포함하고 있지 않습니다. <br>\n",
    "\n",
    "따라서 실습의 원활한 진행을 위해 다른 벤치마크 STS 데이터셋은 korSTS 데이터셋을 다운로드 및 로드하여 사용하도록 하겠습니다.<br>\n",
    "\n",
    "(* 두 데이터셋은 제작 과정이 엄밀히 다르므로, KLUE STS 데이터에 대해 학습된 모델이 KorSTS 테스트셋에 대해 기록하는 점수은 사실상 큰 의미가 없을 수 있습니다. 전체적인 훈련 프로세스의 이해를 돕기 위해 사용한다고 생각해주시는게 좋습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9412e757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02db106841d4685add14b9355774f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1781.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f11f023c484f16b1609213d17a9927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1371.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset kor_nlu/sts (download: 1.53 MiB, generated: 1.54 MiB, post-processed: Unknown size, total: 3.07 MiB) to /workspace/.cache/huggingface/datasets/kor_nlu/sts/1.0.0/4facbba77df60b0658056ced2052633e681a50187b9428bd5752ebd59d332ba8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d148fe7026c46bfa2f3c806ab9e3f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=282044.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba05b8373b844f4997e823b9bf18913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=89895.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef5184af00e4bd587111cfd424e9707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=66129.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset kor_nlu downloaded and prepared to /workspace/.cache/huggingface/datasets/kor_nlu/sts/1.0.0/4facbba77df60b0658056ced2052633e681a50187b9428bd5752ebd59d332ba8. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "testsets = load_dataset(\"kor_nlu\",\"sts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351d6d8",
   "metadata": {},
   "source": [
    "KorSTS 데이터셋은 훈련, 검증 그리고 테스트셋을 지니고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d912e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'genre', 'id', 'score', 'sentence1', 'sentence2', 'year'],\n",
       "        num_rows: 5703\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['filename', 'genre', 'id', 'score', 'sentence1', 'sentence2', 'year'],\n",
       "        num_rows: 1471\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'genre', 'id', 'score', 'sentence1', 'sentence2', 'year'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73289f21",
   "metadata": {},
   "source": [
    "KorSTS의 예시 데이터도 마찬가지로 두 문장과 두 문장 간 유사도를 지니고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c487651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 2,\n",
       " 'genre': 1,\n",
       " 'id': 24,\n",
       " 'score': 2.5,\n",
       " 'sentence1': '한 소녀가 머리를 스타일링하고 있다.',\n",
       " 'sentence2': '한 소녀가 머리를 빗고 있다.',\n",
       " 'year': 6}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsets['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ff384",
   "metadata": {},
   "source": [
    "이제 앞서 얻어진 데이터셋을 sentence-transformers 훈련 양식에 맞게 변환해주는 작업을 거쳐야 합니다.<br>\n",
    "\n",
    "두 데이터 모두 0점에서 5점 사이의 값으로 유사도가 기록되었기 때문에, 0.0 ~ 1.0 스케일로 정규화 시켜주는 작업을 거치게 됩니다. <br>\n",
    "\n",
    "(*KorSTS 내 테스트셋의 경우 None으로 기록된 문장이 몇 개 존재하여, None을 걸러주는 조건이 추가되었습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c35c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [] #KLUE STS 데이터셋의 \"train\" InputExample 객체가 담기는 배열\n",
    "dev_samples =[] #KLUE STS 데이터셋의 \"validation\" InputExample 객체가 담기는 배열\n",
    "test_samples =[] #KorSTS 데이터셋의 \"test\" InputExample 객체가 담기는 배열\n",
    "\n",
    "#KLUE STS 내 훈련, 검증 데이터 예제 변환\n",
    "for phase in [\"train\",\"validation\"]:\n",
    "    examples = datasets[phase]\n",
    "    \n",
    "    for example in examples:\n",
    "        score = float(example[\"labels\"][\"label\"]) / 5.0 #정규화\n",
    "        \n",
    "        inp_example = InputExample(\n",
    "            texts = [example[\"sentence1\"],example[\"sentence2\"]],\n",
    "            label = score,\n",
    "        )\n",
    "        \n",
    "        if phase == \"validation\":\n",
    "            dev_samples.append(inp_example)\n",
    "        else :\n",
    "            train_samples.append(inp_example)\n",
    "# KorSTS 내 테스트 데이터 예제 변환\n",
    "for example in testsets[\"test\"]:\n",
    "    score = float(example[\"score\"]) / 5.0\n",
    "    \n",
    "    if example[\"sentence1\"] and example[\"sentence2\"]: #None을 걸러주는 조건\n",
    "        inp_example = InputExample(\n",
    "            texts=[example[\"sentence1\"], example[\"sentence2\"]],\n",
    "            label = score,\n",
    "        )\n",
    "        \n",
    "    test_samples.append(inp_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf7ed2",
   "metadata": {},
   "source": [
    "앞선 로직을 통해 각 데이터 예제는 다음과 같이 InputExample 객체로 변환하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fea1bb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.',\n",
       "  '숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.'],\n",
       " 0.74)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[0].texts, train_samples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e448c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['한 소녀가 머리를 스타일링하고 있다.', '한 소녀가 머리를 빗고 있다.'], 0.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples[0].texts, test_samples[0].label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8a5d2",
   "metadata": {},
   "source": [
    "이제 학습에 사용될 DataLoader와 Loss를 설정해주도록 합니다. <br>\n",
    "\n",
    "CosineSimilarityLoss는 입력된 두 문장의 임베딩 간 코사인 유사도와 골드 라벨 간 차이를 통해 계산되게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22eaa781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_samples,\n",
    "    shuffle=True,\n",
    "    batch_size = train_batch_size,\n",
    "    )\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfebc5",
   "metadata": {},
   "source": [
    "모델 검증에 활용할 Evaluator 를 정의해줍니다. <br>\n",
    "\n",
    "앞서 얻어진 검증 데이터를 활용하여, 모델의 문장 임베딩 간 코사인 유사도가 얼마나 골드 라벨에 가까운지 계산하는 역할을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab4edba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    dev_samples,\n",
    "    name=\"sts-dev\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15661a8a",
   "metadata": {},
   "source": [
    "모델 학습에 사용될 Warm up Steps를 설정합니다. <br>\n",
    "\n",
    "다양한 방법으로 스텝 수를 결정할 수 있겠지만, 예제 노트북에서는 원 예제 코드를 따라 훈련 배치 수의 10% 만큼으로 값을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fe88cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = math.ceil(len(train_dataloader)* num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296b7dd",
   "metadata": {},
   "source": [
    "이제 앞서 얻어진 객체, 값들을 가지고 모델의 훈련을 진행합니다. <br>\n",
    "\n",
    "sentence-transformers에서는 다음과 같이 fit 함수를 통해 간단히 모델의 훈련과 검증이 가능합니다. <br>\n",
    "\n",
    "훈련 과정을 통해 매 에폭 마다 얻어지는 체크포인트에 대해 Evaluator 가 학습된 모들의 코사인 유사도와 골드 라벨 간 피어슨,스피어만 상관 계수를 계산해 기록을 남기게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a123adf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c0b4d32b6e42438dce7b477397d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef56524bba904e0e8b4f18c3c2299b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1a12e49a8425e9f815bfe9e25191b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0610d13f16df40e0b498fb5afa61d62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf42c422bf0543ff8ea7d69b80c7d341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader,train_loss)],\n",
    "    evaluator = evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24ea41",
   "metadata": {},
   "source": [
    "학습이 완료되었다면 이제 학습된 모델을 테스트 할 시간입니다. <br>\n",
    "\n",
    "앞서 KorSTS 데이터를 활용해 구축한 테스트 데이터셋을 앞서와 마찬가지로 Evaluator로 초기화 해주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce1f74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples,name='sts-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4569c",
   "metadata": {},
   "source": [
    "이제 테스트 Evaluator를 활용하여 테스트셋에 대해 각 상관 계수를 계산하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce9ecfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764882393780215"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator(model,output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8dcc6",
   "metadata": {},
   "source": [
    "역시 검증 데이터에 비해 좋지 않은 점수를 기록하였습니다. <br>\n",
    "\n",
    "KLUE 내 검증 데이터셋 중 일부를 샘플링하여 테스트셋으로 활용하는 방안도 있겠지만,<br>\n",
    "\n",
    "본 노트북은 전체 훈련 프로세스를 파악하는데 초점을 맞추었으므로 실험을 마치도록 합니다.<br>\n",
    "\n",
    "# Sentence Transformers 활용\n",
    "\n",
    "\n",
    "## 시맨틱 서치\n",
    "\n",
    "입력된 문장 간 유사도를 쉽고 빠르게 구할 수 있도록 설계된 sentence-transfromers를 이용한다면 임베딩을 활용해 다양한 어플리케이션을 고안할 수 있습니다.<br>\n",
    "\n",
    "먼저 여러 문장 후보군이 주어졌을 때, 입력된 문장과 가장 유사한 문장을 계산하는 예제를 살펴보도록 합시다. <br>\n",
    "\n",
    "이를 위해 검색의 대상이 되는 문장 후보군을 다음과 같이 정의할 필요가 있습니다. 이후, 정의된 문장 후보군을 미리 임베딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05bac535",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"1992년 7월 8일 손흥민은 강원도 춘천시 후평동에서 아버지 손웅정과 어머니 길은자의 차남으로 태어나 그곳에서 자랐다.\",\n",
    "    \"형은 손흥윤이다.\",\n",
    "    \"춘천 부안초등학교를 졸업했고, 춘천 후평중학교에 입학한 후 2학년때 원주 육민관중학교 축구부에 들어가기 위해 전학하여 졸업하였으며, 2008년 당시 FC 서울의 U-18팀이었던 동북고등학교 축구부에서 선수 활동 중 대한축구협회 우수선수 해외유학 프로젝트에 선발되어 2008년 8월 독일 분데스리가의 함부르크 유소년팀에 입단하였다.\",\n",
    "    \"함부르크 유스팀 주전 공격수로 2008년 6월 네덜란드에서 열린 4개국 경기에서 4게임에 출전, 3골을 터뜨렸다.\",\n",
    "    \"1년간의 유학 후 2009년 8월 한국으로 돌아온 후 10월에 개막한 FIFA U-17 월드컵에 출전하여 3골을 터트리며 한국을 8강으로 이끌었다.\",\n",
    "    \"그해 11월 함부르크의 정식 유소년팀 선수 계약을 체결하였으며 독일 U-19 리그 4경기 2골을 넣고 2군 리그에 출전을 시작했다.\",\n",
    "    \"독일 U-19 리그에서 손흥민은 11경기 6골, 2부 리그에서는 6경기 1골을 넣으며 재능을 인정받아 2010년 6월 17세의 나이로 함부르크의 1군 팀 훈련에 참가, 프리시즌 활약으로 함부르크와 정식 계약을 한 후 10월 18세에 함부르크 1군 소속으로 독일 분데스리가에 데뷔하였다.\",\n",
    "]\n",
    "document_embeddings = model.encode(docs) #문장 후보군 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d2423",
   "metadata": {},
   "source": [
    "이제 입력 문장을 임베딩 할 차례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd93a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"손흥민은 어린 나이에 유럽에 진출하였다.\"\n",
    "query_embedding = model.encode(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa7ac5",
   "metadata": {},
   "source": [
    "아래는 입력된 문장의 임베딩과 미리 임베딩 된 후보군 문장 임베딩 간 유삳호를 계산해 유사도가 높은 순서대로 top_k 개 문장을 뽑아주는 예제 코드입니다.<br>\n",
    "\n",
    "top_k는 전체 문장 후보군의 개수를 넘지 않아야 하므로, min() 함수를 통해 예외 처리를 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eb59285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: 손흥민은 어린 나이에 유럽에 진출하였다.\n",
      "\n",
      "<입력 문장과 유사한 5 개의 문장>\n",
      "\n",
      "1: 독일 U-19 리그에서 손흥민은 11경기 6골, 2부 리그에서는 6경기 1골을 넣으며 재능을 인정받아 2010년 6월 17세의 나이로 함부르크의 1군 팀 훈련에 참가, 프리시즌 활약으로 함부르크와 정식 계약을 한 후 10월 18세에 함부르크 1군 소속으로 독일 분데스리가에 데뷔하였다. (유사도: 0.5482)\n",
      "\n",
      "2: 그해 11월 함부르크의 정식 유소년팀 선수 계약을 체결하였으며 독일 U-19 리그 4경기 2골을 넣고 2군 리그에 출전을 시작했다. (유사도: 0.4281)\n",
      "\n",
      "3: 1992년 7월 8일 손흥민은 강원도 춘천시 후평동에서 아버지 손웅정과 어머니 길은자의 차남으로 태어나 그곳에서 자랐다. (유사도: 0.3384)\n",
      "\n",
      "4: 함부르크 유스팀 주전 공격수로 2008년 6월 네덜란드에서 열린 4개국 경기에서 4게임에 출전, 3골을 터뜨렸다. (유사도: 0.3246)\n",
      "\n",
      "5: 춘천 부안초등학교를 졸업했고, 춘천 후평중학교에 입학한 후 2학년때 원주 육민관중학교 축구부에 들어가기 위해 전학하여 졸업하였으며, 2008년 당시 FC 서울의 U-18팀이었던 동북고등학교 축구부에서 선수 활동 중 대한축구협회 우수선수 해외유학 프로젝트에 선발되어 2008년 8월 독일 분데스리가의 함부르크 유소년팀에 입단하였다. (유사도: 0.2970)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k = min(5, len(docs))\n",
    "\n",
    "# 입력 문장 - 문장 후보군 간 코사인 유사도 계산 후,\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]\n",
    "\n",
    "# 코사인 유사도 순으로 `top_k` 개 문장 추출\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "print(f\"입력 문장: {query}\")\n",
    "print(f\"\\n<입력 문장과 유사한 {top_k} 개의 문장>\\n\")\n",
    "\n",
    "for i, (score, idx) in enumerate(zip(top_results[0], top_results[1])):\n",
    "    print(f\"{i+1}: {docs[idx]} {'(유사도: {:.4f})'.format(score)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f9658",
   "metadata": {},
   "source": [
    "반환된 문장 중 유럽,어린 나이 등의 키워드가 없음에도 높은 유사도로 두 문장이 반환된 것을 확인할 수 있습니다.\n",
    "\n",
    "# 클러스터링\n",
    "\n",
    "sentence-transformers를 통해 얻어진 임베딩을 활용해 클러스터링을 수행할 수도 있습니다. <br>\n",
    "\n",
    "다양한 클러스터링 기법의 적용이 가능하겠지만, 본 노트북에서는 k-Means 클러스터링을 수행한 결과를 살펴보도록 합니다. [k-Means 클러스터링 참고](https://yganalyst.github.io/ml/ML_clustering/#:~:text=%EA%B5%B0%EC%A7%91%EB%B6%84%EC%84%9D(clustering)%EC%9D%B4%EB%9E%80%20%EA%B0%9C%EC%B2%B4,%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0\\)%ED%99%94%ED%95%98%EB%8A%94%20%EB%B0%A9%EB%B2%95%EC%9D%B4%EB%8B%A4. \"k-Means 클러스터링 참고\") <br>\n",
    "\n",
    "예제 수행을 위해 scikit-learn의 설치가 추가로 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f76d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cb4d5",
   "metadata": {},
   "source": [
    "마찬가지로 앞서 구축한 문장 후보군들에 대해 임베딩을 수행합니다. <br>\n",
    "\n",
    "이후, num_clusters 변수를 통해 클러스터의 개수를 설정한 후 임베딩을 활용한 k-Means 클러스터링을 수행하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fccba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings = model.encode(docs) # 후보군 임베딩\n",
    "\n",
    "num_clusters = 3 # 클러스터(개체군)의 개수\n",
    "\n",
    "k_means = KMeans(n_clusters=num_clusters)\n",
    "k_means.fit(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749c278",
   "metadata": {},
   "source": [
    "이제 클러스터링을 통해 각 문장이 어떤 클러스터에 포함되었는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe183925",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dbc5925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54e611",
   "metadata": {},
   "source": [
    "클러스터링 결과를 토대로 각 문장을 클러스터로 분리한 후, 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed40edba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 클러스터 1 >\n",
      "함부르크 유스팀 주전 공격수로 2008년 6월 네덜란드에서 열린 4개국 경기에서 4게임에 출전, 3골을 터뜨렸다.\n",
      "그해 11월 함부르크의 정식 유소년팀 선수 계약을 체결하였으며 독일 U-19 리그 4경기 2골을 넣고 2군 리그에 출전을 시작했다.\n",
      "독일 U-19 리그에서 손흥민은 11경기 6골, 2부 리그에서는 6경기 1골을 넣으며 재능을 인정받아 2010년 6월 17세의 나이로 함부르크의 1군 팀 훈련에 참가, 프리시즌 활약으로 함부르크와 정식 계약을 한 후 10월 18세에 함부르크 1군 소속으로 독일 분데스리가에 데뷔하였다.\n",
      "\n",
      "< 클러스터 2 >\n",
      "1992년 7월 8일 손흥민은 강원도 춘천시 후평동에서 아버지 손웅정과 어머니 길은자의 차남으로 태어나 그곳에서 자랐다.\n",
      "춘천 부안초등학교를 졸업했고, 춘천 후평중학교에 입학한 후 2학년때 원주 육민관중학교 축구부에 들어가기 위해 전학하여 졸업하였으며, 2008년 당시 FC 서울의 U-18팀이었던 동북고등학교 축구부에서 선수 활동 중 대한축구협회 우수선수 해외유학 프로젝트에 선발되어 2008년 8월 독일 분데스리가의 함부르크 유소년팀에 입단하였다.\n",
      "1년간의 유학 후 2009년 8월 한국으로 돌아온 후 10월에 개막한 FIFA U-17 월드컵에 출전하여 3골을 터트리며 한국을 8강으로 이끌었다.\n",
      "\n",
      "< 클러스터 3 >\n",
      "형은 손흥윤이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 클러스터 개수 만큼 문장을 담을 리스트 초기화\n",
    "clustered_sentences = [[] for _ in range(num_clusters)]\n",
    "\n",
    "# 클러스터링 결과를 돌며 각 클러스터에 맞게 문장 삽입\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(docs[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    result = \"\\n\".join(cluster)\n",
    "    print(f\"< 클러스터 {i+1} >\\n{result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8fb6e",
   "metadata": {},
   "source": [
    "국내 관련 문장과 해외 관련 문장, 가족 관계와 같은 느낌으로 클러스터가 형성된 것을 확인할 수 있습니다.\n",
    "\n",
    "지금까지 sentence-transformers를 학습하는 과정을 KLUE STS 데이터셋을 통해 알아보았습니다.\n",
    "\n",
    "sentence-transformers는 다양한 문장 임베딩 기법과 이를 활용한 응용 사례에 대해서 크게 고민하는 UKPLab에서 관리되는 라이브러리이니 만큼 앞으로 더 발전하고 관리될 가능성이 높은 도구입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218bf20",
   "metadata": {},
   "source": [
    "\\[참고자료\\] : https://github.com/UKPLab/sentence-transformers (sentence-transformers git)<br>\n",
    "             https://github.com/Huffon/klue-transformers-tutorial/blob/master/sentence_transformers.ipynb (klue-transformers-tutorial) <br>\n",
    "             https://ainize.ai (ainize) <br>\n",
    "             https://hobinjeong.medium.com/cnn%EC%97%90%EC%84%9C-pooling%EC%9D%B4%EB%9E%80-c4e01aa83c83(pooling 참고)<br>\n",
    "             https://www.youtube.com/watch?v=MiKh_vEZcTY(word Embedding 참고)<br>\n",
    "             https://github.com/KLUE-benchmark/KLUE (dataset)<br>     https://yganalyst.github.io/ml/ML_clustering/#:~:text=%EA%B5%B0%EC%A7%91%EB%B6%84%EC%84%9D(clustering)%EC%9D%B4%EB%9E%80%20%EA%B0%9C%EC%B2%B4,%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0)%ED%99%94%ED%95%98%EB%8A%94%20%EB%B0%A9%EB%B2%95%EC%9D%B4%EB%8B%A4. (K-Means 클러스터링 참고)\n",
    "             \n",
    "             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
